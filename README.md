# diffusion-visual-reasoners

Access the transformed-GQA dataset we built (each example contains a caption and the associated GQA image id) [here](https://drive.google.com/drive/folders/13A_R-97PBbzYkrXj0qt7ILq_tYPhkeMt?usp=share_link).

It contains ~4496 examples with spatially robust captions, generated with the help of GPT-3. We are in the process of cleaning this dataset using heuristics to weed out low-quality captions. 
See our paper, **Diffusion Models as Visual Reasoners**, for more details on how we created the dataset. 



See more details on [GQA](https://cs.stanford.edu/people/dorarad/gqa/about.html).
